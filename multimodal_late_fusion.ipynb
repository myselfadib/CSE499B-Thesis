{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f4602c7-0cfd-49e8-bd91-8268c0c5c0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\envs\\tf\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the VGGish model for audio feature extraction\n",
    "# # Load VGGish model\n",
    "vggish_model = hub.load('https://www.kaggle.com/models/google/vggish/TensorFlow2/vggish/1')\n",
    "\n",
    "def load_and_preprocess_audio(file_path, target_duration=3, target_sr=44100):\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    if sr != target_sr:\n",
    "        audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
    "    target_length = int(target_sr * target_duration)\n",
    "    if len(audio) < target_length:\n",
    "        audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "    else:\n",
    "        audio = audio[:target_length]\n",
    "    embeddings = vggish_model(audio)\n",
    "    return embeddings.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b6a1279-9447-44b7-89e9-dd6558913868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your saved audio model based on the provided directory\n",
    "model_path = \"E:/cse499b/vggish_model/vggish_multimodal_correction.h5\"\n",
    "\n",
    "# Load the trained audio model from the specified path\n",
    "audio_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff89470-fbe6-4cdd-8b95-6c95583b2707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_audio_probabilities(audio_files):\n",
    "    audio_features = np.array([load_and_preprocess_audio(file) for file in audio_files])\n",
    "    probabilities = audio_model.predict(audio_features)\n",
    "    return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a016b5d8-0b08-4f27-abd9-efc5e97e8b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "Predicted probabilities: [[0.31195778 0.05451614 0.08916745 0.45903382 0.08532491]\n",
      " [0.02022971 0.0137376  0.02984658 0.00514909 0.931037  ]]\n"
     ]
    }
   ],
   "source": [
    "# Example audio file paths\n",
    "audio_test_files = ['audio_mod/maa/maa_adib.wav', 'audio_mod/sahajjo/sahajjo_adib.wav']\n",
    "audio_predictions = predict_audio_probabilities(audio_test_files)\n",
    "print(\"Predicted probabilities:\", audio_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6927b958-b866-41bf-a6c9-f71c736e08a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "# # Load the trained frame model from the specified path\n",
    "# frame_model_path = 'E:/cse499b/correction_model/final_model_openpose_v1_correction.keras'  # Adjust this path\n",
    "# frame_model = load_model(frame_model_path)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Define the path to the trained frame model\n",
    "frame_model_path = 'frames_model_five/sign_language_final_model_correction.keras'  # Adjust this path as necessary\n",
    "frame_model = load_model(frame_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "818d40bd-2353-4499-84b8-31f356def216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_video_sequence(video_path, sequence_length=30, img_width=64, img_height=64):\n",
    "    \"\"\"\n",
    "    Load video frames, ensuring we collect the exact sequence length expected by the model.\n",
    "    \"\"\"\n",
    "    frames = sorted([os.path.join(video_path, fname) for fname in os.listdir(video_path)])\n",
    "    if len(frames) < sequence_length:\n",
    "        print(\"Warning: Not enough frames in video to match the expected sequence length.\")\n",
    "        return None  # Or consider padding strategies\n",
    "\n",
    "    # Select the first 'sequence_length' frames\n",
    "    frames = frames[:sequence_length]\n",
    "    frame_array = np.array([img_to_array(load_img(frame, target_size=(img_width, img_height))) for frame in frames])\n",
    "    frame_array = np.expand_dims(frame_array, axis=0)  # Add an extra dimension for batch (model expects batches)\n",
    "    return frame_array / 255.0  # Normalize the frames\n",
    "\n",
    "def predict_video_probabilities(video_paths):\n",
    "    \"\"\"\n",
    "    Predict probabilities for a list of video paths.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for video_path in video_paths:\n",
    "        video_sequence = load_and_preprocess_video_sequence(video_path)\n",
    "        if video_sequence is not None:\n",
    "            probabilities = frame_model.predict(video_sequence)\n",
    "            predictions.append(probabilities[0])  # Append the first (and only) batch result\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfe180a8-29fd-428b-97e4-ece1d6375c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Predicted probabilities: [[5.1959499e-04 9.9889529e-01 1.4265205e-05 3.7874738e-04 1.9207477e-04]\n",
      " [5.2799780e-05 1.0947231e-03 1.8673112e-04 2.1462294e-05 9.9864417e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "video_test_paths = ['E:/openpose_ten/bondhu/p12_c_bondhu.mp4', 'E:/openpose_ten/sahajjo/p9_f_sahajjo.mp4']\n",
    "video_predictions = predict_video_probabilities(video_test_paths)\n",
    "print(\"Predicted probabilities:\", video_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bc80dd1-320e-446a-aacd-ce4394ad06c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Example usage\n",
    "# audio_test_files = ['audio_mod/dhonnobad/dhonnobad_mahim.wav', 'audio_mod/sahajjo/sahajjo_adib.wav']\n",
    "# video_test_paths = ['E:/openpose_ten/dhonnobad/p1_c_dhonnobad.mp4', 'E:/openpose_ten/sahajjo/p9_f_sahajjo.mp4']\n",
    "\n",
    "# # Call the late fusion function\n",
    "# fused_probabilities, final_predictions = late_fusion(audio_test_files, video_test_paths)\n",
    "# print(\"Fused probabilities:\", fused_probabilities)\n",
    "# print(\"Final predictions:\", final_predictions)\n",
    "# Manually specified test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de260d6b-3e54-4b14-a8a5-87891d22f9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Fused probabilities: [[0.7568264  0.07439683 0.13500236 0.01966042 0.01411393]\n",
      " [0.77646065 0.05356031 0.09787168 0.01633846 0.05576886]\n",
      " [0.79978013 0.02658044 0.05066443 0.07921567 0.04375933]\n",
      " [0.8794732  0.05420192 0.05776419 0.0071047  0.001456  ]\n",
      " [0.04396368 0.84204185 0.09462544 0.00124948 0.0181196 ]\n",
      " [0.11137923 0.69948155 0.16632809 0.00720104 0.0156101 ]\n",
      " [0.24521986 0.5834948  0.15328607 0.00969109 0.00830818]\n",
      " [0.03555528 0.8287591  0.08540072 0.00318142 0.04710347]\n",
      " [0.1771257  0.14612508 0.6626418  0.00618302 0.00792441]\n",
      " [0.04865779 0.17579496 0.69725204 0.00844794 0.06984726]\n",
      " [0.0686364  0.05220177 0.61089945 0.02814474 0.24011761]\n",
      " [0.11841594 0.07369731 0.7522058  0.00522858 0.0504524 ]\n",
      " [0.20059611 0.04168527 0.03210189 0.7098869  0.01572977]\n",
      " [0.15602443 0.02727307 0.0446092  0.72943074 0.04266258]\n",
      " [0.02581884 0.00629813 0.00855234 0.91544324 0.04388744]\n",
      " [0.116859   0.09987986 0.24229228 0.5079598  0.03300905]\n",
      " [0.00137144 0.00369775 0.00315089 0.00349718 0.9882827 ]\n",
      " [0.05054208 0.09109063 0.1955186  0.00435993 0.6584887 ]\n",
      " [0.01012415 0.00710803 0.01494402 0.00257857 0.9652452 ]\n",
      " [0.00988338 0.04377082 0.03074372 0.01893677 0.89666533]]\n",
      "Final predictions: [0 0 0 0 1 1 1 1 2 2 2 2 3 3 3 3 4 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "def late_fusion(audio_test_files, video_test_paths):\n",
    "    # Predict probabilities using the audio model\n",
    "    audio_probs = predict_audio_probabilities(audio_test_files)\n",
    "    \n",
    "    # Predict probabilities using the frame model\n",
    "    video_probs = predict_video_probabilities(video_test_paths)\n",
    "    \n",
    "    # Ensure both predictions are of the same length\n",
    "    if len(audio_probs) != len(video_probs):\n",
    "        raise ValueError(\"The number of audio and video predictions must match\")\n",
    "    \n",
    "    # Perform averaging of probabilities from both models\n",
    "    fused_probs = (audio_probs + video_probs) / 2\n",
    "    \n",
    "    # Determine final predictions based on maximum probability\n",
    "    final_predictions = np.argmax(fused_probs, axis=1)\n",
    "    \n",
    "    return fused_probs, final_predictions\n",
    "\n",
    "audio_test_files = [\n",
    "    'audio_mod/baba/baba_mahim.wav',\n",
    "    'audio_mod/baba/baba_adib.wav',\n",
    "    'audio_mod/baba/baba_ramim.wav',\n",
    "    'audio_mod/baba/baba_riaz.wav',\n",
    "    'audio_mod/bondhu/bondhu_adib.wav',\n",
    "    'audio_mod/bondhu/bondhu_mahim.wav',\n",
    "    'audio_mod/bondhu/bondhu_riaz.wav',\n",
    "    'audio_mod/bondhu/bondhu_ramim.wav',\n",
    "    'audio_mod/dhonnobad/dhonnobad_adib.wav',\n",
    "    'audio_mod/dhonnobad/dhonnobad_mahim.wav',\n",
    "    'audio_mod/dhonnobad/dhonnobad_ramim.wav',\n",
    "    'audio_mod/dhonnobad/dhonnobad_riaz.wav',\n",
    "    'audio_mod/maa/maa_riaz.wav',\n",
    "    'audio_mod/maa/maa_adib.wav',\n",
    "    'audio_mod/maa/maa_ramim.wav',\n",
    "    'audio_mod/maa/maa_mahim.wav',\n",
    "    'audio_mod/sahajjo/sahajjo_ramim.wav',\n",
    "    'audio_mod/sahajjo/sahajjo_riaz.wav',\n",
    "    'audio_mod/sahajjo/sahajjo_adib.wav',\n",
    "    'audio_mod/sahajjo/sahajjo_mahim.wav'\n",
    "]\n",
    "video_test_paths = [\n",
    "    'E:/openpose_ten/baba/p1_c_baba.mp4',\n",
    "    'E:/openpose_ten/baba/p10_c_baba.mp4',\n",
    "    'E:/openpose_ten/baba/p7_c_baba.mp4',\n",
    "    'E:/openpose_ten/baba/p11_c_baba.mp4',\n",
    "    'E:/openpose_ten/bondhu/p1_c_bondhu.mp4',\n",
    "    'E:/openpose_ten/bondhu/p10_c_bondhu.mp4',\n",
    "    'E:/openpose_ten/bondhu/p5_c_bondhu.mp4',\n",
    "    'E:/openpose_ten/bondhu/p7_c_bondhu.mp4',\n",
    "    'E:/openpose_ten/dhonnobad/p1_c_dhonnobad.mp4',\n",
    "    'E:/openpose_ten/dhonnobad/p10_c_dhonnobad.mp4',\n",
    "    'E:/openpose_ten/dhonnobad/p11_c_dhonnobad.mp4',\n",
    "    'E:/openpose_ten/dhonnobad/p9_c_dhonnobad.mp4',\n",
    "    'E:/openpose_ten/maa/p1_c_maa.mp4',\n",
    "    'E:/openpose_ten/maa/p10_c_maa.mp4',\n",
    "    'E:/openpose_ten/maa/p7_c_maa.mp4',\n",
    "    'E:/openpose_ten/maa/p11_c_maa.mp4',\n",
    "    'E:/openpose_ten/sahajjo/p9_f_sahajjo.mp4',\n",
    "    'E:/openpose_ten/sahajjo/p1_c_sahajjo.mp4',\n",
    "    'E:/openpose_ten/sahajjo/p5_c_sahajjo.mp4',\n",
    "    'E:/openpose_ten/sahajjo/p6_c_sahajjo.mp4'\n",
    "]\n",
    "\n",
    "# Call the late fusion function\n",
    "fused_probabilities, final_predictions = late_fusion(audio_test_files, video_test_paths)\n",
    "print(\"Fused probabilities:\", fused_probabilities)\n",
    "print(\"Final predictions:\", final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "344fcb07-9a04-407e-9bea-3481eb2a558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_labels = [2, 4, 2, 4]  # Adjust these labels according to your actual class assignment\n",
    "\n",
    "# # Call the late fusion function\n",
    "# fused_probabilities, final_predictions = late_fusion(audio_test_files, video_test_paths)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# accuracy = accuracy_score(true_labels, final_predictions)\n",
    "# print(\"Accuracy of the model:\", accuracy)\n",
    "\n",
    "# # Get a classification report\n",
    "# report = classification_report(true_labels, final_predictions, target_names=['Class 0', 'Class 1','Class 2','Class 3','Class 4'])\n",
    "# print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# # Define labels for all classes expected in the model\n",
    "# all_class_labels = [0, 1, 2, 3, 4]  # Assuming classes are 0-indexed and you have 5 classes total\n",
    "\n",
    "# # Provide names for these classes (if these are specific signs, replace 'Class X' with the actual sign names)\n",
    "# target_names = ['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4']\n",
    "\n",
    "# # Make sure the true labels are correct based on your description\n",
    "# true_labels = [2, 4, 2, 4]  # Adjust these labels if necessary\n",
    "\n",
    "# # Assuming you have already executed your late fusion and obtained 'final_predictions'\n",
    "# # Call the classification report with all classes\n",
    "# report = classification_report(\n",
    "#     true_labels, final_predictions,\n",
    "#     labels=all_class_labels,\n",
    "#     target_names=target_names\n",
    "# )\n",
    "\n",
    "# print(\"Classification Report:\\n\", report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6089fca-1267-4d15-87a5-623cbe61c699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 1.0\n",
      "Recall of the model: 1.0\n",
      "F1 Score of the model: 1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      1.00      1.00         4\n",
      "     Class 1       1.00      1.00      1.00         4\n",
      "     Class 2       1.00      1.00      1.00         4\n",
      "     Class 3       1.00      1.00      1.00         4\n",
      "     Class 4       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# True labels for the test data\n",
    "true_labels = [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4]  # Adjust these as needed to match your data\n",
    "\n",
    "# Calculate accuracy, recall, and F1-score\n",
    "accuracy = accuracy_score(true_labels, final_predictions)\n",
    "recall = recall_score(true_labels, final_predictions, average='macro')\n",
    "f1 = f1_score(true_labels, final_predictions, average='macro')\n",
    "\n",
    "print(\"Accuracy of the model:\", accuracy)\n",
    "print(\"Recall of the model:\", recall)\n",
    "print(\"F1 Score of the model:\", f1)\n",
    "\n",
    "# Define labels and target names for all classes in the model\n",
    "all_class_labels = [0, 1, 2, 3, 4]  # Assuming classes are 0-indexed and you have 5 classes total\n",
    "target_names = ['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4']\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(\n",
    "    true_labels, final_predictions,\n",
    "    labels=all_class_labels,\n",
    "    target_names=target_names\n",
    ")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa44934-911d-4361-b3c5-0d7638e503b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
